{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Final Version of ADL-HW5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKraYPnw4Vm5",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation \n",
        "  - Author: Sabrina Li\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "552QcaE33U1K",
        "colab_type": "code",
        "outputId": "427c8a5d-6c55-43fb-b324-dbb08537eb15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 332.1MB 45kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 6.8MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 419kB 11.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qXyJxYD593j",
        "colab_type": "code",
        "outputId": "bdd3a4f2-fe7b-468f-be73-874a92fabbdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install sacrebleu # https://github.com/mjpost/sacreBLEU"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/12/5b/7196b11bca204cb6ca9000b5dc910e809081f224c73ef28e9991080e4e51/sacrebleu-1.3.1.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Building wheels for collected packages: sacrebleu\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/56/c0/fb/1c7f9b3a71f64cdf86291cc645596f71746807bf2f72b3c1dd\n",
            "Successfully built sacrebleu\n",
            "Installing collected packages: sacrebleu\n",
            "Successfully installed sacrebleu-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtYXP-576PSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import sacrebleu\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import unicodedata\n",
        "import os\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KkAcYosALVB",
        "colab_type": "text"
      },
      "source": [
        "## 1. Train a model to translate from English to Spanish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xs-Zjnj7N66",
        "colab_type": "code",
        "outputId": "aa21b4df-ea0a-4b24-ea30-bbed5f3a9ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC4k6fgl6gEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud51qUfR7gn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "    np.random.seed(123)\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    lines = np.reshape(np.array(lines), (int(len(lines)/2), 2))\n",
        "    indexes = np.random.choice(lines.shape[0], num_examples)\n",
        "    selected_lines = np.array([lines[i] for i in indexes]).flatten()\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in selected_lines]\n",
        "    return word_pairs, zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF5-S3Lj7hS2",
        "colab_type": "code",
        "outputId": "174825cd-ae89-4304-9820-9ad0ce518c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "num_sample = 3000\n",
        "sentences, (en, sp) = create_dataset(path_to_file, num_sample)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> over ten thousand messages are sent every second on facebook . <end>\n",
            "<start> mas de diez mil mensajes son enviados cada segundo en facebook . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBKycWvF6hrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"Original:\", sentences[0])\n",
        "#sentences = [(preprocess_sentence(source), preprocess_sentence(target)) for (source, target) in sentences]\n",
        "#print(\"Preprocessed:\", sentences[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRheDyz677X9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_sentences, target_sentences = en, sp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWbWYrUIAdxt",
        "colab_type": "code",
        "outputId": "6249a77b-49eb-43d0-b3d0-d2869ef3ac47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_tokenizer.fit_on_texts(source_sentences)\n",
        "source_data = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "print(\"Sequence:\", source_data[0])\n",
        "source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='post')\n",
        "print(\"Padded:\", source_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence: [1, 27, 12, 39, 25, 678, 3, 23, 15, 91, 9, 213, 3, 2]\n",
            "Padded: [  1  27  12  39  25 678   3  23  15  91   9 213   3   2   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB8nZ8TUCGwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_tokenizer.fit_on_texts(target_sentences)\n",
        "target_data = target_tokenizer.texts_to_sequences(target_sentences)\n",
        "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhWMWEbe05Hb",
        "colab_type": "code",
        "outputId": "2abd083e-4cbb-4a67-a190-850754d8fda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Create labels for the decoder by shifting the target sequence\n",
        "# one to the right.\n",
        "target_labels = np.zeros(target_data.shape)\n",
        "target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]\n",
        "\n",
        "print(\"Target sequence\", target_data[0])\n",
        "print(\"Target label\", target_labels[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target sequence [  1  26   8  13  23 597   3  13  53  22 529   3   2   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0]\n",
            "Target label [ 26.   8.  13.  23. 597.   3.  13.  53.  22. 529.   3.   2.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8G7XznrGQY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir6teLD0DVib",
        "colab_type": "code",
        "outputId": "eca6fda3-6a10-464d-c5ae-66da1f3a8ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def decode(encoded, tokenizer):\n",
        "  for number in encoded:\n",
        "    if number !=0:\n",
        "      print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n",
        "      \n",
        "decode(source_data[0], source_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 -> <start>\n",
            "27 -> mary\n",
            "12 -> is\n",
            "39 -> not\n",
            "25 -> my\n",
            "678 -> girlfriend\n",
            "3 -> .\n",
            "23 -> she\n",
            "15 -> s\n",
            "91 -> just\n",
            "9 -> a\n",
            "213 -> friend\n",
            "3 -> .\n",
            "2 -> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf5yfHccETFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels)).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYNaoDqSFSrr",
        "colab_type": "code",
        "outputId": "3acce0b3-3a9f-4bd5-e535-9534e3dfa346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_batch = next(iter(dataset))\n",
        "source, target, taget_labels = example_batch\n",
        "print(\"Shapes:\", source.shape, target.shape, taget_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes: (5, 29) (5, 26) (5, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqU008dQFd73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 32\n",
        "rnn_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9IBpuqAGAQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(source_vocab_size,\n",
        "                                               embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)        \n",
        "    return output, state\n",
        "  \n",
        "  def init_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, rnn_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3UJBp40K2aq",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate calling the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMjL3nN2GBfw",
        "colab_type": "code",
        "outputId": "5610de4d-a4a3-4b37-866f-5dd4cda1e0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Create a batch of one sentence\n",
        "ex_sentence = tf.expand_dims(source_data[0], axis=0)\n",
        "ex_translation = tf.expand_dims(target_data[0], axis=0)\n",
        "ex_labels = tf.expand_dims(target_labels[0], axis=0)\n",
        "print(ex_sentence.shape)\n",
        "\n",
        "encoder = Encoder()\n",
        "hidden_state = encoder.init_state(batch_size=1)\n",
        "print(hidden_state.shape)\n",
        "\n",
        "output, hidden_state = encoder(ex_sentence, hidden_state)\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 29)\n",
            "(1, 64)\n",
            "(1, 29, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nONasBCIC6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, \n",
        "                                               embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUNFCk-ZVdgT",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate calling the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCiY1TjLMZ1T",
        "colab_type": "code",
        "outputId": "ebfaedcb-d3ae-412e-e0e2-41f92114e800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder()\n",
        "decoder_output, decoder_state = decoder(ex_labels, hidden_state)\n",
        "print(decoder_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 26, 5424)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhMLKqtkNA6p",
        "colab_type": "code",
        "outputId": "183e652e-3b59-4682-8dcc-63b4561f3450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def calc_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  return crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "print(\"Loss\", calc_loss(ex_labels, decoder_output))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss tf.Tensor(3.9689026, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS-tPiEGTkVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(idx=None, input_sent=None):\n",
        "  \n",
        "    if idx == None and input_sent == None: \n",
        "      idx = np.random.choice(len(sentences))\n",
        "      out_input_sent = sentences[idx][0]\n",
        "      input_sent = source_data[idx]\n",
        "      out_target_sent = sentences[idx][1]\n",
        "    elif idx == None and input_sent != None:\n",
        "      out_target_sent = \"NOT AVAILABLE\"\n",
        "      out_input_sent = input_sent\n",
        "      #print(out_input_sent)\n",
        "      source_data2 = source_tokenizer.texts_to_sequences(out_input_sent)\n",
        "      #print(source_data2)\n",
        "      source_data2 = tf.keras.preprocessing.sequence.pad_sequences(source_data2, padding='post')\n",
        "      input_sent = source_data2\n",
        "      input_sent = np.array(input_sent).reshape(len(input_sent),)\n",
        "      #print(input_sent)\n",
        "    else:\n",
        "      out_input_sent = sentences[idx][0]\n",
        "      input_sent = source_data[idx]\n",
        "      out_target_sent = sentences[idx][1]\n",
        "    #input_sent = source_data[idx]\n",
        "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "    \n",
        "    hidden_state = encoder.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder(input_sent, hidden_state)\n",
        "    \n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "    \n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "      \n",
        "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "        # before the decoder is trained, just stop translating and return\n",
        "        # what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "          \n",
        "    translation = ' '.join(out_words)    \n",
        "    return out_input_sent, out_target_sent, translation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAR0qOfdWQWH",
        "colab_type": "code",
        "outputId": "f2018e9b-e1db-4b0c-b1bc-6927a01edcf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "input_sent, target_sent, translation = translate()\n",
        "print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> that s not how the world works . <end>\n",
            "Target: <start> asi no es como funciona el mundo . <end>\n",
            "Translation: verme refran comida on amas acuerdo entregale favor charlando apoderan velas averguenza metro papa encontramos moriras perdio darle notara jazz\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MUQX_Ltssut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEr-jDSpZ_Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function # remove this annotation when debugging\n",
        "def train_step(source_seq, target_seq, target_labels, initial_state):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_output, encoder_state = encoder(source_seq, initial_state)\n",
        "    logits, decoder_state = decoder(target_seq, encoder_state)\n",
        "    loss = calc_loss(target_labels, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjIbuxscstrQ",
        "colab_type": "code",
        "outputId": "d9b56da0-be09-4696-cf5f-238fd7c614d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2822
        }
      },
      "source": [
        "EPOCHS = 400\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "  \n",
        "    en_initial_states = encoder.init_state(batch_size)\n",
        "    \n",
        "    for batch, (source_seq, target_seq, target_labels) in enumerate(dataset):\n",
        "      #print(source_seq.shape, target_seq.shape, target_labels.shape)\n",
        "      loss = train_step(source_seq, target_seq, target_labels, en_initial_states)\n",
        "      elapsed = time.time() - start\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      print(\"Epoch #%d, Loss %.4f, Time %.2f sec\" % (epoch, loss, elapsed))\n",
        "      input_sent, target_sent, translation = translate()\n",
        "      print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #0, Loss 2.0388, Time 13.94 sec\n",
            "Input: <start> the king reigned over the island . <end>\n",
            "Target: <start> el rey reino sobre la isla . <end>\n",
            "Translation: tom que no que no que no que no que no que no que no que no que no que\n",
            "\n",
            "Epoch #10, Loss 1.1885, Time 10.71 sec\n",
            "Input: <start> she was indignant at the way she had been treated . <end>\n",
            "Target: <start> ella se indigno por la forma en que fue tratada . <end>\n",
            "Translation: ella se quedo a mary . <end>\n",
            "\n",
            "Epoch #20, Loss 0.8260, Time 10.68 sec\n",
            "Input: <start> the statue of liberty is the symbol of the united states . <end>\n",
            "Target: <start> la estatua de la libertad es el simbolo de america . <end>\n",
            "Translation: la estatua de la libertad es el simbolo de america . <end>\n",
            "\n",
            "Epoch #30, Loss 0.5542, Time 10.69 sec\n",
            "Input: <start> the pot is boiling over . <end>\n",
            "Target: <start> la olla esta hirviendo . <end>\n",
            "Translation: la vida es un buen extranjero por nuestro pais . <end>\n",
            "\n",
            "Epoch #40, Loss 0.4851, Time 10.72 sec\n",
            "Input: <start> when did you see her last ? <end>\n",
            "Target: <start> ¿ cuando la viste por ultima vez ? <end>\n",
            "Translation: ¿ cuando fue el afortunado ? <end>\n",
            "\n",
            "Epoch #50, Loss 0.4043, Time 10.74 sec\n",
            "Input: <start> we will make an exception of your case . <end>\n",
            "Target: <start> haremos una excepcion en su caso . <end>\n",
            "Translation: nosotros jugamos los platos . <end>\n",
            "\n",
            "Epoch #60, Loss 0.3104, Time 11.47 sec\n",
            "Input: <start> call me when you re ready . <end>\n",
            "Target: <start> llamenme cuando esten listos . <end>\n",
            "Translation: llamenme cuando esten listos . <end>\n",
            "\n",
            "Epoch #70, Loss 0.2700, Time 10.69 sec\n",
            "Input: <start> i was a student at that time . <end>\n",
            "Target: <start> yo era un estudiante entonces . <end>\n",
            "Translation: yo tambien tengo sueno . <end>\n",
            "\n",
            "Epoch #80, Loss 0.1920, Time 12.05 sec\n",
            "Input: <start> montgomery was where it all began . <end>\n",
            "Target: <start> donde todo surgio fue en montgomery . <end>\n",
            "Translation: donde todos los asientos pasan de navidad ? <end>\n",
            "\n",
            "Epoch #90, Loss 0.1736, Time 10.66 sec\n",
            "Input: <start> money isn t everything . <end>\n",
            "Target: <start> el dinero no lo es todo . <end>\n",
            "Translation: el cuchillo esta romo . <end>\n",
            "\n",
            "Epoch #100, Loss 0.2705, Time 11.58 sec\n",
            "Input: <start> let s move on . <end>\n",
            "Target: <start> avancemos . <end>\n",
            "Translation: quedemonos aca . <end>\n",
            "\n",
            "Epoch #110, Loss 0.1336, Time 11.38 sec\n",
            "Input: <start> my birthday is in november . <end>\n",
            "Target: <start> mi cumpleanos es en noviembre . <end>\n",
            "Translation: mary fue la puerta con su madre adoptiva . <end>\n",
            "\n",
            "Epoch #120, Loss 0.1023, Time 10.77 sec\n",
            "Input: <start> this is the house where i live . <end>\n",
            "Target: <start> esta es la casa donde vivo . <end>\n",
            "Translation: eso es ridiculo . <end>\n",
            "\n",
            "Epoch #130, Loss 0.0619, Time 10.68 sec\n",
            "Input: <start> tom bought a bunch of grapes and i ate them . <end>\n",
            "Target: <start> tom compro un monton de uvas y me las comi . <end>\n",
            "Translation: tom podia adivinar por la sonrisa en su cara de mary de caso . <end>\n",
            "\n",
            "Epoch #140, Loss 0.0862, Time 10.76 sec\n",
            "Input: <start> i need a hammer to nail the boards . <end>\n",
            "Target: <start> necesito un martillo para clavar las tablas . <end>\n",
            "Translation: necesito una camisa de cuello duro . <end>\n",
            "\n",
            "Epoch #150, Loss 0.0720, Time 10.67 sec\n",
            "Input: <start> a friend of mine called on me at my office . <end>\n",
            "Target: <start> un amigo me llamo a mi oficina . <end>\n",
            "Translation: un vaso de agua , mis labios estan sellados . <end>\n",
            "\n",
            "Epoch #160, Loss 0.0359, Time 11.28 sec\n",
            "Input: <start> he will come afterwards . <end>\n",
            "Target: <start> el vendra mas tarde . <end>\n",
            "Translation: el es asertiva . <end>\n",
            "\n",
            "Epoch #170, Loss 0.0677, Time 10.74 sec\n",
            "Input: <start> tom politely pretended not to notice that mary had been crying . <end>\n",
            "Target: <start> educadamente , tom fingio no notar que mary habia estado llorando . <end>\n",
            "Translation: educadamente , tom fingio no notar que mary habia estado llorando . <end>\n",
            "\n",
            "Epoch #180, Loss 0.0430, Time 10.81 sec\n",
            "Input: <start> i hope you like the job . <end>\n",
            "Target: <start> espero que te guste el trabajo . <end>\n",
            "Translation: espero que te guste el trabajo . <end>\n",
            "\n",
            "Epoch #190, Loss 0.0378, Time 11.79 sec\n",
            "Input: <start> i m just a boy who makes mistakes . <end>\n",
            "Target: <start> solo soy un nino que comete errores . <end>\n",
            "Translation: solo soy un martillo del mire . <end>\n",
            "\n",
            "Epoch #200, Loss 0.0492, Time 11.74 sec\n",
            "Input: <start> we ll need a head hunting agency to find the right man for this executive position . <end>\n",
            "Target: <start> necesitamos una agencia caza talentos para encontrar al hombre adecuado para este puesto ejecutivo . <end>\n",
            "Translation: necesitamos dinero . <end>\n",
            "\n",
            "Epoch #210, Loss 0.0377, Time 10.69 sec\n",
            "Input: <start> the boat was approaching the english channel . <end>\n",
            "Target: <start> el barco se acercaba al canal de la mancha . <end>\n",
            "Translation: el barco se habla acerca de usted de que el se fuera . <end>\n",
            "\n",
            "Epoch #220, Loss 0.0387, Time 10.78 sec\n",
            "Input: <start> she cremated him within hours of his death . <end>\n",
            "Target: <start> ella lo cremo a horas de su muerte . <end>\n",
            "Translation: ella le susurro algo . <end>\n",
            "\n",
            "Epoch #230, Loss 0.0321, Time 10.82 sec\n",
            "Input: <start> a mother s love is unconditional . <end>\n",
            "Target: <start> el amor de madre es incondicional . <end>\n",
            "Translation: el amor de madre esta sola . <end>\n",
            "\n",
            "Epoch #240, Loss 0.0213, Time 11.21 sec\n",
            "Input: <start> i m kind of hungry , too . <end>\n",
            "Target: <start> pues yo tambien tengo hambre . <end>\n",
            "Translation: he estado son musica de la mesa . <end>\n",
            "\n",
            "Epoch #250, Loss 0.0225, Time 10.74 sec\n",
            "Input: <start> tom has one foot in the grave . <end>\n",
            "Target: <start> tom esta a punto de morder el polvo . <end>\n",
            "Translation: tom conoce el problema . <end>\n",
            "\n",
            "Epoch #260, Loss 0.0377, Time 10.76 sec\n",
            "Input: <start> tom wanted to find a creative way to tell mary that he loved her . <end>\n",
            "Target: <start> tom queria encontrar una manera creativa de decir a mary que el la amaba . <end>\n",
            "Translation: tom queria que mary le contara sobre su infancia . <end>\n",
            "\n",
            "Epoch #270, Loss 0.0306, Time 12.06 sec\n",
            "Input: <start> tom s fingerprint was evidence that he was there . <end>\n",
            "Target: <start> la huella digital de tom era evidencia de que el estaba ahi . <end>\n",
            "Translation: la huella digital de lujo por no . <end>\n",
            "\n",
            "Epoch #280, Loss 0.0241, Time 10.76 sec\n",
            "Input: <start> sounds good to me . <end>\n",
            "Target: <start> a mi me suena bien . <end>\n",
            "Translation: a mi persona no sabe que un nuevo y boston en el clase . <end>\n",
            "\n",
            "Epoch #290, Loss 0.0617, Time 10.84 sec\n",
            "Input: <start> if you need any help , ask me . <end>\n",
            "Target: <start> preguntame si necesitas ayuda . <end>\n",
            "Translation: preguntame si necesitas ayuda . <end>\n",
            "\n",
            "Epoch #300, Loss 0.0694, Time 10.75 sec\n",
            "Input: <start> our family name will be ruined if tom is convicted . <end>\n",
            "Target: <start> nuestro apellido se echara a perder si tom es condenado . <end>\n",
            "Translation: nuestro agua hace anos mas sola . <end>\n",
            "\n",
            "Epoch #310, Loss 0.0358, Time 10.74 sec\n",
            "Input: <start> tom was drinking at a bar with mary at the time the police think that john was murdered . <end>\n",
            "Target: <start> tom estaba bebiendo en un bar con mary en el momento en que la policia cree que john fue asesinado . <end>\n",
            "Translation: tom casi siempre se bana antes de cenar . <end>\n",
            "\n",
            "Epoch #320, Loss 0.0137, Time 10.76 sec\n",
            "Input: <start> there is no way of reaching the island other than by boat . <end>\n",
            "Target: <start> no hay otro modo de llegar a la isla mas que por bote . <end>\n",
            "Translation: ¿ que te has comprado tu tren ? <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQKaLOyY9TDE",
        "colab_type": "text"
      },
      "source": [
        "Calculate BLEU Score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACRaHldF4QCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "references, hypotheses = [], []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  input_sent, target_sent, translation = translate()\n",
        "  references.append(target_sent)\n",
        "  hypotheses.append(\"<start> \" + translation)\n",
        "  \n",
        "results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81g7Ec7PAeqy",
        "colab_type": "text"
      },
      "source": [
        "### 2. Train a second model to translate between the same two languages in reverse order (from Spanish to English)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_RlBDu_hiLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_sample = 3000\n",
        "sentences, (en, sp) = create_dataset(path_to_file, num_sample)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cagbnhlCRzEJ",
        "colab": {}
      },
      "source": [
        "source_sentences_re, target_sentences_re = sp, en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I_sulZZVRzEQ",
        "colab": {}
      },
      "source": [
        "source_tokenizer_re = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_tokenizer_re.fit_on_texts(source_sentences_re)\n",
        "source_data_re = source_tokenizer_re.texts_to_sequences(source_sentences_re)\n",
        "print(\"Sequence:\", source_data_re[1])\n",
        "source_data_re = tf.keras.preprocessing.sequence.pad_sequences(source_data_re, padding='post')\n",
        "print(\"Padded:\", source_data_re[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lqbZUtElRzEa",
        "colab": {}
      },
      "source": [
        "target_tokenizer_re = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_tokenizer_re.fit_on_texts(target_sentences_re)\n",
        "target_data_re = target_tokenizer_re.texts_to_sequences(target_sentences_re)\n",
        "target_data_re = tf.keras.preprocessing.sequence.pad_sequences(target_data_re, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mtos_G6LRzEl",
        "colab": {}
      },
      "source": [
        "# Create labels for the decoder by shifting the target sequence\n",
        "# one to the right.\n",
        "target_labels_re = np.zeros(target_data_re.shape)\n",
        "target_labels_re[:,0:target_data_re.shape[1] -1] = target_data_re[:,1:]\n",
        "\n",
        "print(\"Target sequence\", target_data_re[0])\n",
        "print(\"Target label\", target_labels_re[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ElOQnXgMRzEw",
        "colab": {}
      },
      "source": [
        "source_vocab_size_re = len(source_tokenizer_re.word_index) + 1\n",
        "target_vocab_size_re = len(target_tokenizer_re.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vTigJip5RzE0",
        "colab": {}
      },
      "source": [
        "def decode_re(encoded, tokenizer):\n",
        "  for number in encoded:\n",
        "    if number !=0:\n",
        "      print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n",
        "      \n",
        "decode_re(source_data_re[0], source_tokenizer_re)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t1Mh5dNpRzE4",
        "colab": {}
      },
      "source": [
        "batch_size = 5\n",
        "target_labels_re = tf.convert_to_tensor(target_labels_re)\n",
        "dataset_re = tf.data.Dataset.from_tensor_slices((source_data_re, target_data_re,target_labels_re)).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n3jO2zixRzFE",
        "colab": {}
      },
      "source": [
        "example_batch = next(iter(dataset_re))\n",
        "source, target, taget_labels = example_batch\n",
        "print(\"Shapes:\", source.shape, target.shape, taget_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JIv6CuRoRzFJ",
        "colab": {}
      },
      "source": [
        "embedding_size_re = 32\n",
        "rnn_size_re = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UjWW_td-RzFK",
        "colab": {}
      },
      "source": [
        "class Encoder_re(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Encoder_re, self).__init__()\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(source_vocab_size_re,\n",
        "                                               embedding_size_re)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size_re, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)        \n",
        "    return output, state\n",
        "  \n",
        "  def init_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, rnn_size_re))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ODJ_6hvnRzFM"
      },
      "source": [
        "Demonstrate calling the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "shgbtrWcRzFO",
        "colab": {}
      },
      "source": [
        "# Create a batch of one sentence\n",
        "ex_sentence_re = tf.expand_dims(source_data_re[0], axis=0)\n",
        "ex_translation_re = tf.expand_dims(target_data_re[0], axis=0)\n",
        "ex_labels_re = tf.expand_dims(target_labels_re[0], axis=0)\n",
        "print(ex_sentence_re.shape)\n",
        "\n",
        "encoder_re = Encoder_re()\n",
        "hidden_state_re = encoder.init_state(batch_size=1)\n",
        "print(hidden_state_re.shape)\n",
        "\n",
        "output_re, hidden_state_re = encoder(ex_sentence_re, hidden_state_re)\n",
        "print(output_re.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jwoLMaPjRzFS",
        "colab": {}
      },
      "source": [
        "class Decoder_re(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder_re, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size_re, \n",
        "                                               embedding_size_re)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_size_re, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(target_vocab_size_re)\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lJGTLLLCRzFT"
      },
      "source": [
        "Demonstrate calling the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0dvKCw6GRzFT",
        "colab": {}
      },
      "source": [
        "decoder_re = Decoder_re()\n",
        "decoder_output_re, decoder_state_re = decoder_re(ex_labels_re, hidden_state_re)\n",
        "print(decoder_output_re.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UEDTnR4qRzFW",
        "colab": {}
      },
      "source": [
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def calc_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  return crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "print(\"Loss\", calc_loss(ex_labels_re, decoder_output_re))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mPUl_hiGTw0G",
        "colab": {}
      },
      "source": [
        "def translate_re(idx=None, input_sent=None):\n",
        "  \n",
        "    if idx == None and input_sent == None: \n",
        "      idx = np.random.choice(len(sentences))\n",
        "      input_sent = source_data_re[idx]\n",
        "      out_input_sent = sentences[idx][1]\n",
        "      out_target_sent = sentences[idx][0]\n",
        "    elif idx == None and input_sent != None:\n",
        "      out_input_sent = input_sent\n",
        "      #input_sent = input_sent.strip('<start>')\n",
        "      #input_sent = input_sent.strip('<end>')\n",
        "      \n",
        "      input_sent = preprocess_sentence(input_sent)\n",
        "      #print(input_sent.split(' '))\n",
        "      inputs = [source_tokenizer_re.word_index[i] for i in input_sent.split(' ')[2:-2]]\n",
        "      input_sent = tf.keras.preprocessing.sequence.pad_sequences([inputs], padding='post')\n",
        "      #input_sent = tf.convert_to_tensor(inputs)\n",
        "      input_sent = input_sent[0]\n",
        "      out_target_sent = \"UNKNOWN\"\n",
        "      \n",
        "      \n",
        "      '''\n",
        "      idx = np.random.choice(len(sentences))\n",
        "      \n",
        "      source_data2 = source_tokenizer_re.texts_to_sequences(input_sent)\n",
        "      source_data2 = tf.keras.preprocessing.sequence.pad_sequences(source_data2, padding='post')\n",
        "      input_sent = source_data2\n",
        "      input_sent = np.array(input_sent).reshape(len(input_sent),)\n",
        "      #print(input_sent)\n",
        "      '''\n",
        "      \n",
        "      #out_target_sent = sentences[idx][0]     \n",
        "    else:\n",
        "      out_input_sent = sentences[idx][1]\n",
        "      #source_data2 = source_tokenizer_re.texts_to_sequences(out_input_sent)\n",
        "      #source_data2 = tf.keras.preprocessing.sequence.pad_sequences(source_data2, padding='post')\n",
        "      input_sent = source_data_re[idx]\n",
        "      #print(input_sent)\n",
        "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "    \n",
        "    hidden_state = encoder_re.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder_re(input_sent, hidden_state)\n",
        "    decoder_input = tf.expand_dims([target_tokenizer_re.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "    \n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "      \n",
        "        decoder_output, decoder_state = decoder_re(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "        # before the decoder is trained, just stop translating and return\n",
        "        # what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer_re.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "          \n",
        "    translation = ' '.join(out_words)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return out_input_sent, out_target_sent, translation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B6Ketx-8Tw0O",
        "colab": {}
      },
      "source": [
        "input_sent, target_sent, translation = translate_re()\n",
        "print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_F4FKvLPTw0Z",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DXL-7ug8Tw0o",
        "colab": {}
      },
      "source": [
        "@tf.function # remove this annotation when debugging\n",
        "def train_step_re(source_seq_re, target_seq_re, target_labels_re, initial_state_re):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_output_re, encoder_state_re = encoder_re(source_seq_re, initial_state_re)\n",
        "    logits, decoder_state_re = decoder_re(target_seq_re, encoder_state_re)\n",
        "    loss = calc_loss(target_labels_re, logits)\n",
        "\n",
        "  variables = encoder_re.trainable_variables + decoder_re.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oXg2mNhVTw0q",
        "colab": {}
      },
      "source": [
        "EPOCHS = 400\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "  \n",
        "    en_initial_states_re = encoder_re.init_state(batch_size)\n",
        "    \n",
        "    for batch, (source_seq_re, target_seq_re, target_labels_re) in enumerate(dataset_re):\n",
        "      #print(source_seq.shape, target_seq.shape, target_labels.shape)\n",
        "      loss = train_step_re(source_seq_re, target_seq_re, target_labels_re, en_initial_states_re)\n",
        "      elapsed = time.time() - start\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      print(\"Epoch #%d, Loss %.4f, Time %.2f sec\" % (epoch, loss, elapsed))\n",
        "      input_sent, target_sent, translation = translate_re()\n",
        "      print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk7F5l0Q_c55",
        "colab_type": "text"
      },
      "source": [
        "Calculate BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRILT6lQUXvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "references, hypotheses = [], []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  input_sent, target_sent, translation = translate_re()\n",
        "  references.append(target_sent)\n",
        "  hypotheses.append(\"<start> \" + translation)\n",
        "  \n",
        "results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "indouRiXArFg",
        "colab_type": "text"
      },
      "source": [
        "### 3. Back-translate. Use the previous two models to translate a sentence from English to Spanish, and then back to English. Compare the original sentence, and the back-translated sentence. Repeat this using an evaluation corpus of 1,000 sentences, and report the BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vWpxrRMd-3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = np.array(sentences)\n",
        "selected_index = np.random.randint(sentences.shape[0], size=100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh6OsBsh_gf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "references, translations, hypotheses = [], [], []\n",
        "\n",
        "for i in selected_index:\n",
        "  #print(sentences[i][0])\n",
        "  input_sent, target_sent, translation = translate(i)\n",
        "  print('input:   ', input_sent)\n",
        "  translation = \"<start>\" + translation\n",
        "  references.append(input_sent)\n",
        "  translations.append(translation)\n",
        "  #print('translation: ', translation)\n",
        "\n",
        "\n",
        "for tr in translations:\n",
        "\n",
        "  \n",
        "  input_sent, target_sent, new_translation = translate_re(input_sent= tr)\n",
        "  #print('input from translation_re: ', input_sent)\n",
        "  #print('translation from translation_re: ', translation)\n",
        "  print('new translation from translation_re: ', new_translation)\n",
        "  hypotheses.append(\"<start> \" + new_translation + \" <end>)\n",
        "  \n",
        "results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "print(results)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRFP9XeFiu9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}